# job_scrabing_yes123.py

import re
import time
import traceback
import sqlite3

from urllib.parse import urlencode, quote_plus, urljoin
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException, WebDriverException

DB_PATH = "my_database.db"
MAX_ITEMS = 1000   # æœ€å¤šæŠ“ï¼å¯«å…¥ä¸Šé™

def create_database():
    """å»ºç«‹ yes123 è·ç¼ºè³‡æ–™è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute('''
        CREATE TABLE IF NOT EXISTS job_listings_yes123 (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            job_title TEXT NOT NULL,
            tools TEXT,
            skills TEXT,
            company TEXT,
            job_name TEXT,
            update_time TEXT,
            source TEXT DEFAULT 'yes123',
            UNIQUE(job_title, company)
        )
    ''')
    conn.commit()
    conn.close()

def clear_table():
    """æ¯æ¬¡åŸ·è¡Œå‰æ¸…ç©ºè¡¨ï¼Œè®“ id å¾ 1 é–‹å§‹"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute('DELETE FROM job_listings_yes123;')
    conn.commit()
    conn.close()

def build_search_url(name: str, offset: int=0) -> str:
    """å¯å¸¶ offset åˆ° strrec åƒæ•¸åšåˆ†é """
    params = {
        'find_key2': name,
        'search_key_word': name,
        'order_ascend': 'desc',
        'strrec': offset,
        'search_type': 'job',
        'search_from': 'joblist',
    }
    return f"https://www.yes123.com.tw/wk_index/joblist.asp?{urlencode(params, quote_via=quote_plus)}"

def webloading_list(timeout=10):
    """åˆ—è¡¨é å°ˆç”¨ç­‰å¾…ï¼šç­‰åˆ°å‡ºç¾ Job_opening_item"""
    end = time.time() + timeout
    while time.time() < end:
        if "Job_opening_item" in driver.page_source:
            return
        time.sleep(0.3)
    raise TimeoutException("åˆ—è¡¨é è¼‰å…¥è¶…æ™‚")

def webloading_detail(timeout=10):
    """è©³ç´°é å°ˆç”¨ç­‰å¾…ï¼šç­‰åˆ° document.readyState å®Œæˆ"""
    end = time.time() + timeout
    while time.time() < end:
        if driver.execute_script("return document.readyState") == "complete":
            return
        time.sleep(0.3)
    raise TimeoutException("è©³æƒ…é è¼‰å…¥è¶…æ™‚")

def extract_job_data(url):
    """æ“·å–è©³ç´°é è³‡æ–™ï¼Œå›å‚³ [job_title, tools, skills, company, update_time]"""
    try:
        driver.get(url)
        webloading_detail()
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(0.8)
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        tm = soup.find('time')
        dt = tm['datetime'] if tm and tm.has_attr('datetime') else ''

        tools = []
        hdr = soup.find('h3', string=lambda t: t and 'æŠ€èƒ½èˆ‡æ±‚è·å°ˆé•·' in t)
        if hdr:
            for li in hdr.find_next_sibling('ul').find_all('li'):
                sp = li.find('span', class_='right_main')
                if sp:
                    tools.append(sp.get_text(strip=True))

        skills = []
        hdr2 = soup.find('h3', string=lambda t: t and 'å…¶ä»–æ¢ä»¶' in t)
        if hdr2:
            for li in hdr2.find_next_sibling('ul').find_all('li'):
                ex = li.find('span', class_='exception')
                if ex:
                    skills.append(ex.get_text(strip=True))

        return ['', ', '.join(tools), ', '.join(skills), '', dt]

    except Exception:
        traceback.print_exc()
        return None

def scrabing(keyword):
    """
    ä»¥ offset åˆ†é ï¼š
      offset å¾ 0 é–‹å§‹ï¼Œæ¯æ¬¡åŠ ä¸Š page_sizeï¼Œ
      ç›´åˆ°æŠ“ä¸åˆ°ä»»ä½•æ¢ç›®æˆ–ç´¯ç©é” MAX_ITEMSã€‚
    """
    results = []
    offset = 0

    while len(results) < MAX_ITEMS:
        url = build_search_url(keyword, offset)
        print(f"\nâ”€â”€â”€ æŠ“åˆ—è¡¨: offset={offset}")
        driver.get(url)
        webloading_list()

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select('div.Job_opening_item')
        if not items:
            print("â— åˆ—è¡¨æ²’æœ‰æ¢ç›®ï¼ŒçµæŸåˆ†é ")
            break

        for item in items:
            if len(results) >= MAX_ITEMS:
                break

            a = item.select_one('div.Job_opening_item_title h5 a')
            if not a:
                continue
            href = a['href']
            full = urljoin(url, href)
            title = a.get_text(strip=True)
            comp_el = item.select_one('div.Job_opening_item_title h6 a')
            comp = comp_el.get_text(strip=True) if comp_el else ''
            date_div = item.select_one(
                'div.Job_opening_item_date div.d-flex > div:nth-of-type(2)'
            )
            up = date_div.get_text(strip=True) if date_div else ''

            detail = extract_job_data(full)
            if detail and detail[1].strip():
                # å¡«å›åˆ—è¡¨æ¨™é¡Œï¼å…¬å¸ï¼åˆ—è¡¨æ›´æ–°æ™‚é–“
                detail[0], detail[3], detail[4] = title, comp, up
                results.append(detail)
                # ==== å³æ™‚åˆ—å° ====
                print(f"[å³æ™‚] ç¬¬{len(results):03d}ç­† â†’ {detail}")

        offset += len(items)
        time.sleep(1.0)

    print(f">>> å…±æ“·å–åˆ° {len(results)} ç­†ï¼ˆå«è©³æƒ…ï¼‰")
    return results

def store_in_database(data, job_name, write_count):
    """æŠŠå–®ç­†è³‡æ–™å¯«å…¥ SQLiteï¼Œä¸¦æª¢æŸ¥ä¸Šé™"""
    if write_count >= MAX_ITEMS:
        return False
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute(
        'INSERT OR REPLACE INTO job_listings_yes123 '
        '(job_title, tools, skills, company, job_name, update_time, source) '
        'VALUES (?, ?, ?, ?, ?, ?, ?)',
        (data[0], data[1], data[2], data[3], job_name, data[4], 'yes123')
    )
    conn.commit()
    conn.close()
    # ==== å³æ™‚åˆ—å°å¯«å…¥ ====
    print(f"[å·²å­˜] ç¬¬{write_count+1:03d}ç­† â†’ {data}")
    return True

def run_yes123_scraping(keywords=None):
    """yes123 çˆ¬èŸ²ä¸»æµç¨‹ï¼ˆå«æ¸…ç©ºè¡¨èˆ‡å³æ™‚åˆ—å°ï¼‰"""
    global driver
    create_database()
    clear_table()

    if not keywords:
        keywords = ['è»Ÿé«”']
    driver = webdriver.Chrome()
    driver.set_window_size(1920, 1080)

    write_count = 0
    for kw in keywords:
        print(f"\nâ–¶ é–‹å§‹çˆ¬å–ï¼š{kw}")
        for rec in scrabing(kw):
            if not store_in_database(rec, kw, write_count):
                print(f"ğŸ”” å·²å¯«å…¥ {MAX_ITEMS} ç­†ï¼Œä¸Šé™é”æˆï¼Œåœæ­¢å­˜å…¥ã€‚")
                driver.quit()
                return
            write_count += 1

    driver.quit()
    print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±å¯«å…¥ {write_count} ç­†è³‡æ–™")

if __name__ == '__main__':
    run_yes123_scraping()
