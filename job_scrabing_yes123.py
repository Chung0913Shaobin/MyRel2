# job_scrabing_yes123.py

import re
import time
import traceback
import sqlite3

from urllib.parse import urlencode, quote_plus, urljoin
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException, WebDriverException

DB_PATH = "my_database.db"
MAX_ITEMS = 1000   # æœ€å¤šæŠ“ï¼å¯«å…¥ä¸Šé™

def create_database():
    """å»ºç«‹ yes123 è·ç¼ºè³‡æ–™è¡¨ï¼šå…ˆ DROP å†é‡å»ºï¼ˆæ–°å¢ location æ¬„ä½ï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute('DROP TABLE IF EXISTS job_listings_yes123;')
    cur.execute('''
        CREATE TABLE job_listings_yes123 (
            id           INTEGER PRIMARY KEY AUTOINCREMENT,
            job_title    TEXT    NOT NULL,
            tools        TEXT,
            skills       TEXT,
            company      TEXT,
            job_name     TEXT,
            update_time  TEXT,
            location     TEXT,               -- æ–°å¢å·¥ä½œåœ°é»æ¬„ä½
            source       TEXT    DEFAULT 'yes123',
            UNIQUE(job_title, company)
        )
    ''')
    conn.commit()
    conn.close()

def clear_table():
    """æ¸…ç©ºè¡¨æ ¼ï¼ˆè‹¥ä½ é‚„æƒ³ä¿ç•™ DROP å¯ä»¥çœç•¥é€™æ®µï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute('DELETE FROM job_listings_yes123;')
    conn.commit()
    conn.close()

def build_search_url(name: str, offset: int=0) -> str:
    params = {
        'find_key2': name,
        'search_key_word': name,
        'order_ascend': 'desc',
        'strrec': offset,
        'search_type': 'job',
        'search_from': 'joblist',
    }
    return f"https://www.yes123.com.tw/wk_index/joblist.asp?{urlencode(params, quote_via=quote_plus)}"

def webloading_list(timeout=10):
    end = time.time() + timeout
    while time.time() < end:
        if "Job_opening_item" in driver.page_source:
            return
        time.sleep(0.3)
    raise TimeoutException("åˆ—è¡¨é è¼‰å…¥è¶…æ™‚")

def webloading_detail(timeout=10):
    end = time.time() + timeout
    while time.time() < end:
        if driver.execute_script("return document.readyState") == "complete":
            return
        time.sleep(0.3)
    raise TimeoutException("è©³æƒ…é è¼‰å…¥è¶…æ™‚")

def extract_job_data(url):
    """
    æ“·å–è©³ç´°é è³‡æ–™ï¼Œå›å‚³ï¼š
      [job_title, tools, skills, company, update_time, location]
    """
    try:
        driver.get(url)
        webloading_detail()
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(0.8)
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # æŠ“å–æ›´æ–°æ™‚é–“
        tm = soup.find('time')
        dt = tm['datetime'] if tm and tm.has_attr('datetime') else ''

        # æ“…é•·å·¥å…·
        tools = []
        hdr = soup.find('h3', string=lambda t: t and 'æŠ€èƒ½èˆ‡æ±‚è·å°ˆé•·' in t)
        if hdr:
            for li in hdr.find_next_sibling('ul').find_all('li'):
                sp = li.find('span', class_='right_main')
                if sp:
                    tools.append(sp.get_text(strip=True))

        # å…¶ä»–æ¢ä»¶ => skills
        skills = []
        hdr2 = soup.find('h3', string=lambda t: t and 'å…¶ä»–æ¢ä»¶' in t)
        if hdr2:
            for li in hdr2.find_next_sibling('ul').find_all('li'):
                ex = li.find('span', class_='exception')
                if ex:
                    skills.append(ex.get_text(strip=True))

        # ==== æ–°å¢ï¼šå·¥ä½œåœ°é» ====
        location = ""
        loc_span = soup.find('span', class_='left_title', string=lambda t: t and 'å·¥ä½œåœ°é»' in t)
        if loc_span:
            right_span = loc_span.find_next_sibling('span', class_='right_main')
            if right_span:
                # ç”¨ç©ºæ ¼ä¸²æ¥æ‰€æœ‰æ–‡å­—ï¼Œå»é™¤å¤šé¤˜æ›è¡Œ
                location = right_span.get_text(separator=' ', strip=True).replace('\n', ' ')
                location = ' '.join(location.split())

        # å›å‚³ç©º job_title ç”±å¤–å±¤å¡«å…¥
        return ['', ', '.join(tools), ', '.join(skills), '', dt, location]

    except Exception:
        traceback.print_exc()
        return None

def scrabing(keyword):
    results = []
    offset = 0

    while len(results) < MAX_ITEMS:
        url = build_search_url(keyword, offset)
        print(f"\nâ”€â”€â”€ æŠ“åˆ—è¡¨: offset={offset}")
        driver.get(url)
        webloading_list()

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        items = soup.select('div.Job_opening_item')
        if not items:
            print("â— åˆ—è¡¨æ²’æœ‰æ¢ç›®ï¼ŒçµæŸåˆ†é ")
            break

        for item in items:
            if len(results) >= MAX_ITEMS:
                break

            a = item.select_one('div.Job_opening_item_title h5 a')
            if not a:
                continue
            href = a['href']
            full = urljoin(url, href)
            title = a.get_text(strip=True)

            comp_el = item.select_one('div.Job_opening_item_title h6 a')
            comp = comp_el.get_text(strip=True) if comp_el else ''

            date_div = item.select_one(
                'div.Job_opening_item_date div.d-flex > div:nth-of-type(2)'
            )
            up = date_div.get_text(strip=True) if date_div else ''

            detail = extract_job_data(full)
            if detail and detail[1].strip():
                # å¡«å›åˆ—è¡¨ç´šçš„ job_title/company/update_time
                detail[0], detail[3], detail[4] = title, comp, up
                results.append(detail)
                print(f"[å³æ™‚] ç¬¬{len(results):03d}ç­† â†’ {detail}")

        offset += len(items)
        time.sleep(1.0)

    print(f">>> å…±æ“·å–åˆ° {len(results)} ç­†ï¼ˆå«è©³æƒ…ï¼‰")
    return results

def store_in_database(data, job_name, write_count):
    """æŠŠå–®ç­†è³‡æ–™å¯«å…¥ SQLiteï¼ŒåŒ…å« location"""
    if write_count >= MAX_ITEMS:
        return False
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute(
        'INSERT OR REPLACE INTO job_listings_yes123 '
        '(job_title, tools, skills, company, job_name, update_time, location, source) '
        'VALUES (?, ?, ?, ?, ?, ?, ?, ?)',
        (data[0], data[1], data[2], data[3], job_name, data[4], data[5], 'yes123')
    )
    conn.commit()
    conn.close()
    print(f"[å·²å­˜] ç¬¬{write_count+1:03d}ç­† â†’ {data}")
    return True

def run_yes123_scraping(keywords=None):
    """yes123 çˆ¬èŸ²ä¸»æµç¨‹ï¼ˆé‡å»ºè¡¨ã€æ¸…ç©ºã€çˆ¬å–ã€å­˜åº«ï¼‰"""
    global driver
    create_database()
    clear_table()

    if not keywords:
        keywords = ['è»Ÿé«”']
    driver = webdriver.Chrome()
    driver.set_window_size(1920, 1080)

    write_count = 0
    for kw in keywords:
        print(f"\nâ–¶ é–‹å§‹çˆ¬å–ï¼š{kw}")
        for rec in scrabing(kw):
            if not store_in_database(rec, kw, write_count):
                print(f"ğŸ”” å·²å¯«å…¥ {MAX_ITEMS} ç­†ï¼Œä¸Šé™é”æˆï¼Œåœæ­¢å­˜å…¥ã€‚")
                driver.quit()
                return
            write_count += 1

    driver.quit()
    print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…±å¯«å…¥ {write_count} ç­†è³‡æ–™")

if __name__ == '__main__':
    run_yes123_scraping()
