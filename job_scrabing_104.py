# job_scrabing_104.py

import re
import time
import traceback
import sqlite3
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import WebDriverException

DB_PATH = "my_database.db"

# ==== ä¿®æ”¹ ==== 
MAX_ITEMS = 1000   # è¨­å®šæœ€å¤šæŠ“å–è·ç¼ºæ•¸é‡ä¸Šé™
# ==== ä¿®æ”¹ ==== 

def create_database():
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute('''
        CREATE TABLE IF NOT EXISTS job_listings_104 (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            job_title TEXT NOT NULL,
            tools TEXT,
            skills TEXT,
            company TEXT,
            job_name TEXT,
            update_time TEXT,
            source TEXT DEFAULT '104',
            UNIQUE(job_title, company)
        )
    ''')
    conn.commit()
    conn.close()
    print("âœ… è³‡æ–™åº« (104) å»ºç«‹å®Œæˆï¼")

def webloading():
    """ç­‰å¾…é é¢å®Œå…¨è¼‰å…¥ï¼Œå†å¤šç­‰ 1 ç§’è®“ JS æ¸²æŸ“çµæŸã€‚"""
    for _ in range(50):
        if driver.execute_script("return document.readyState") == "complete":
            break
        time.sleep(0.1)
    time.sleep(1.0)

def extract_job_data(url):
    """æ“·å–å–®ä¸€è·ç¼ºå…§é çš„æ¨™é¡Œã€å…¬å¸ã€å·¥å…·ã€æŠ€èƒ½ã€æ›´æ–°æ™‚é–“ã€‚"""
    try:
        driver.get(url)
        webloading()
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(0.8)
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # æ¨™é¡Œ
        h1 = soup.find('h1', class_='d-inline')
        title = h1.get_text(strip=True) if h1 else ""

        # å…¬å¸
        comp = soup.find('div', class_='mt-3 pr-6')
        company = comp.find('a')['title'] if comp and comp.find('a') else ""

        # æ›´æ–°æ™‚é–“
        upd = soup.find('div', class_='job-header__title')
        update_time = upd.find('span')['title'] if upd and upd.find('span') else ""

        # tools
        tools = [e.text.strip() for e in driver.find_elements(
            By.CSS_SELECTOR, 'a[class*="tools text-gray-deep-dark"]'
        ) if e.text.strip()]

        # skills
        skills = [e.text.strip() for e in driver.find_elements(
            By.CSS_SELECTOR, 'a[class*="skills text-gray-deep-dark"]'
        ) if e.text.strip()]

        print(f"âœ” æ“·å–: {title} | {company} | å·¥å…·:{tools} | æŠ€èƒ½:{skills} | æ›´æ–°:{update_time}")
        return [title, ", ".join(tools), ", ".join(skills), company, update_time]

    except Exception as e:
        print(f"âŒ æ“·å–å¤±æ•— {url}ï¼š{e}")
        traceback.print_exc()
        return None

def scrabing(keyword):
    """
    é€é æŠ“ï¼šç”¨ &page=N åˆ†é ï¼Œ
    ä»¥æ­£å‰‡åŒ¹é… '/job/' çš„é€£çµï¼Œ
    ç„¡é€£çµæˆ–é”åˆ°ä¸Šé™å³åœæ­¢ã€‚
    """
    results = []
    page = 1

    while True:
        # ==== ä¿®æ”¹ ==== 
        if len(results) >= MAX_ITEMS:
            print(f"ğŸ”” å·²é” {MAX_ITEMS} ç­†ä¸Šé™ï¼Œåœæ­¢æŠ“å–ã€‚")
            break
        # ==== ä¿®æ”¹ ==== 

        url = f"https://www.104.com.tw/jobs/search/?ro=0&kwop=7&keyword={keyword}&page={page}"
        print(f"\nâ”€â”€â”€ æŠ“å– 104 é—œéµå­—ã€Œ{keyword}ã€ç¬¬ {page} é  (å·²æŠ“ {len(results)} ç­†)")
        driver.get(url)
        webloading()

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        candidates = soup.find_all('a', href=re.compile(r'/job/'))
        links = []
        for a in candidates:
            # ==== ä¿®æ”¹ ==== 
            if len(results) >= MAX_ITEMS:
                print(f"ğŸ”” å·²é” {MAX_ITEMS} ç­†ä¸Šé™ï¼Œåœæ­¢æŠ“å–ã€‚")
                break
            # ==== ä¿®æ”¹ ==== 

            href = a.get('href')
            if not href or '/job/' not in href:
                continue
            if href.startswith('//'):
                full = 'https:' + href
            elif href.startswith('http'):
                full = href
            else:
                full = 'https://www.104.com.tw' + href
            full = full.split('?')[0]
            links.append(full)

        # å»é‡
        links = list(dict.fromkeys(links))

        if not links:
            print(f"â— ç¬¬ {page} é ç„¡è·ç¼ºé€£çµï¼ŒçµæŸåˆ†é ã€‚")
            break

        print(f"  â†’ æœ¬é æ‰¾åˆ° {len(links)} å€‹è·ç¼ºé€£çµ")
        for link in links:
            # ==== ä¿®æ”¹ ==== 
            if len(results) >= MAX_ITEMS:
                print(f"ğŸ”” å·²é” {MAX_ITEMS} ç­†ä¸Šé™ï¼Œåœæ­¢æŠ“å–ã€‚")
                break
            # ==== ä¿®æ”¹ ==== 

            data = extract_job_data(link)
            if data:
                results.append(data)

        page += 1
        time.sleep(1.0)

    return results

def store_in_database(data_list, job_name):
    """å°‡æ“·å–çµæœå­˜å…¥ SQLite"""
    if not data_list:
        print(f"[è·³é] ã€{job_name}ã€ç„¡ä»»ä½•çµæœ")
        return
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    for row in data_list:
        cur.execute('''
            INSERT OR REPLACE INTO job_listings_104
            (job_title, tools, skills, company, job_name, update_time, source)
            VALUES (?, ?, ?, ?, ?, ?, '104')
        ''', (*row, job_name))
    conn.commit()
    conn.close()
    print(f"âœ… å­˜å…¥è³‡æ–™åº«ï¼šã€{job_name}ã€å…± {len(data_list)} ç­†")

def run_104_scraping(keywords=None):
    global driver
    create_database()
    if keywords is None:
        keywords = ['è»Ÿé«”']
    driver = webdriver.Chrome()
    driver.set_window_size(1920, 1080)

    for kw in keywords:
        try:
            results = scrabing(kw)
            store_in_database(results, kw)
        except WebDriverException as e:
            print(f"[ERR] Selenium ç™¼ç”Ÿå•é¡Œï¼š{e}")
            traceback.print_exc()

    driver.quit()
    print("\nâ˜†â˜…â˜† 104è·ç¼ºçˆ¬å–å®Œæˆ â˜†â˜…â˜†")

if __name__ == "__main__":
    run_104_scraping()
