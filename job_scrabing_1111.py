# job_scrabing_1111.py

import re
import time
import traceback
import sqlite3
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.common.exceptions import WebDriverException

DB_PATH = "my_database.db"
MAX_ITEMS = 1000   # è¨­å®šæœ€å¤šæŠ“å–è·ç¼ºæ•¸é‡ä¸Šé™

def create_database():
    """å»ºç«‹ 1111 è·ç¼º SQLite è³‡æ–™è¡¨ï¼ˆå…ˆåˆªé™¤å†é‡å»ºï¼Œæ–°å¢ location æ¬„ä½ï¼‰"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute('DROP TABLE IF EXISTS job_listings_1111;')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS job_listings_1111 (
            id           INTEGER PRIMARY KEY AUTOINCREMENT,
            job_title    TEXT    NOT NULL,
            tools        TEXT,
            skills       TEXT,
            company      TEXT,
            job_name     TEXT,
            update_time  TEXT,
            location     TEXT,               -- æ–°å¢åœ°é»æ¬„ä½
            source       TEXT    DEFAULT '1111',
            UNIQUE(job_title, company)
        )
    ''')
    conn.commit()
    conn.close()
    print("âœ… è³‡æ–™åº« (1111) å»ºç«‹å®Œæˆï¼")

def webloading():
    """ç­‰å¾… document.readyState == completeï¼Œå†å¤šç­‰ 1.5 ç§’è®“ JS æ¸²æŸ“çµæŸã€‚"""
    for _ in range(50):
        if driver.execute_script("return document.readyState") == "complete":
            break
        time.sleep(0.1)
    time.sleep(1.5)

def scroll_in_job_page():
    """åœ¨è·ç¼ºå…§é æ²åˆ°åº•ï¼Œç¢ºä¿æ‡¶è¼‰å…¥å€å¡Šéƒ½è§¸ç™¼åˆ°ã€‚"""
    last = driver.execute_script("return document.body.scrollHeight")
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(1.0)
        new = driver.execute_script("return document.body.scrollHeight")
        if new == last:
            break
        last = new

def clean_company_name(name):
    name = re.sub(r'\s*[â€§\-ï¼â€“â€”]\s*\d+\s*å€‹è·ç¼º.*$', '', name)
    name = re.sub(r'\ï¼ˆ.*?\ï¼‰|\(.*?\)', '', name)
    return name.strip()

def get_company_name(soup):
    for a in soup.find_all('a', href=lambda x: x and '/corp/' in x):
        txt = a.get_text(strip=True)
        if txt:
            return clean_company_name(txt)
    span = soup.find('span', class_="inline-block font-medium text-[#2066EC] ml-1 text-[14px]")
    if span and span.get_text(strip=True):
        return clean_company_name(span.get_text(strip=True))
    return "æŸ¥ç„¡å…¬å¸"

def extract_job_data(url, job_title):
    """
    æ“·å–å–®ä¸€è·ç¼ºè©³æƒ…ï¼šå…¬å¸ã€æ›´æ–°æ™‚é–“ã€å·¥å…·ã€æŠ€èƒ½ã€å·¥ä½œåœ°é»ã€‚
    æ¸…ç†æ‰€æœ‰å…§éƒ¨æ›è¡Œï¼Œè®“è¼¸å‡ºä¸€è¡Œé¡¯ç¤ºã€‚
    """
    try:
        driver.get(url)
        webloading()
        scroll_in_job_page()
        soup = BeautifulSoup(driver.page_source, 'html.parser')

        # å…¬å¸åç¨±
        company = get_company_name(soup)

        # æ›´æ–°æ™‚é–“
        tm = soup.find('time')
        update_time = tm['datetime'] if tm and tm.has_attr('datetime') else ""

        # æ“…é•·å·¥å…·
        tools = []
        header = soup.find('h3', string=lambda t: t and "é›»è…¦å°ˆé•·" in t)
        if header:
            ul = header.find_next_sibling(
                'ul',
                attrs={'class': lambda c: c and 'flex' in c.split() and 'flex-wrap' in c.split()}
            )
            if ul:
                for p in ul.find_all(
                    'p',
                    attrs={'class': lambda c: c and 'underline' in c.split() and 'underline-offset-1' in c.split()}
                ):
                    tools.append(p.get_text(separator=' ', strip=True))

        # å·¥ä½œæŠ€èƒ½
        skills = []
        for li in driver.find_elements(
            By.CSS_SELECTOR,
            'li[class*="flex flex-wrap justify-start items-center"]'
        ):
            txt = li.text.replace('\n', ' ').strip()
            if txt:
                skills.append(txt)

        # å·¥ä½œåœ°é»
        location = ""
        loc_header = soup.find('h3', string=lambda t: t and "å·¥ä½œåœ°é»" in t)
        if loc_header:
            div_sib = loc_header.find_next_sibling('div')
            if div_sib:
                p_loc = div_sib.find('p')
                if p_loc:
                    # ç”¨ç©ºæ ¼æ¥çºŒæ‰€æœ‰å­ç¯€é»ï¼Œä¸¦å»æ‰å¤šé¤˜æ›è¡Œ
                    location = p_loc.get_text(separator=' ', strip=True).replace('\n', ' ')
                    # ç¢ºä¿å¤šé‡ç©ºç™½åˆä½µç‚ºä¸€å€‹
                    location = ' '.join(location.split())

        print(
            f"âœ” æ“·å–: {job_title} | å…¬å¸={company} | å·¥å…·={tools} | "
            f"æŠ€èƒ½={skills} | æ›´æ–°={update_time} | åœ°é»={location}"
        )

        return {
            'job_title':   job_title,
            'tools':       ", ".join(tools),
            'skills':      ", ".join(skills),
            'company':     company,
            'update_time': update_time,
            'location':    location
        }

    except Exception as e:
        print(f"âŒ extract_job_data éŒ¯èª¤ ({url})ï¼š{e}")
        traceback.print_exc()
        return None

def scrabing(keyword):
    """
    é€é æŠ“ï¼šç”¨ &page=N åˆ†é ï¼Œ
    æ¯é ç”¨åŸæœ¬ find_all('/job/æ•¸å­—', title=True) æ’ˆé€£çµï¼Œ
    ç„¡é€£çµæˆ–é”åˆ°ä¸Šé™å³åœæ­¢ã€‚
    """
    results = []
    page = 1

    while True:
        if len(results) >= MAX_ITEMS:
            print(f"ğŸ”” å·²é” {MAX_ITEMS} ç­†ä¸Šé™ï¼Œåœæ­¢æŠ“å–ã€‚")
            break

        search_url = f"https://www.1111.com.tw/search/job?ks={keyword}&page={page}"
        print(f"\nâ”€â”€â”€ æŠ“å– 1111 é—œéµå­—ã€Œ{keyword}ã€ç¬¬ {page} é ")
        driver.get(search_url)
        webloading()

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        links = soup.find_all('a', href=re.compile(r'^/job/\d+'), title=True)
        if not links:
            print(f"â— ç¬¬ {page} é ç„¡è·ç¼ºé€£çµï¼ŒçµæŸåˆ†é ã€‚")
            break

        seen = set()
        for a in links:
            if len(results) >= MAX_ITEMS:
                print(f"ğŸ”” å·²é” {MAX_ITEMS} ç­†ä¸Šé™ï¼Œåœæ­¢æŠ“å–ã€‚")
                break
            href = a['href'].split('?')[0]
            full = urljoin("https://www.1111.com.tw", href)
            if full in seen:
                continue
            seen.add(full)
            title = a['title'].strip()
            data = extract_job_data(full, title)
            if data:
                results.append(data)

        page += 1
        time.sleep(1.0)

    return results

def store_in_database(job_list, keyword):
    """æŠŠçµæœå¯«é€² SQLiteï¼ˆåŒ…å« location æ¬„ä½ï¼‰"""
    if not job_list:
        print(f"[è·³é] ã€{keyword}ã€ç„¡ä»»ä½•çµæœ")
        return
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    for item in job_list:
        cur.execute('''
            INSERT OR REPLACE INTO job_listings_1111
            (job_title, tools, skills, company, job_name, update_time, location, source)
            VALUES (?, ?, ?, ?, ?, ?, ?, '1111')
        ''', (
            item['job_title'],
            item['tools'],
            item['skills'],
            item['company'],
            keyword,
            item['update_time'],
            item['location']
        ))
    conn.commit()
    conn.close()
    print(f"âœ… å­˜å…¥è³‡æ–™åº«ï¼šã€{keyword}ã€å…± {len(job_list)} ç­†")

def run_1111_scraping(keywords=None):
    global driver
    create_database()
    if keywords is None:
        keywords = ['è»Ÿé«”']
    driver = webdriver.Chrome()
    driver.set_window_size(1920, 1080)

    for kw in keywords:
        try:
            data = scrabing(kw)
            store_in_database(data, kw)
        except WebDriverException as e:
            print(f"[ERR] Selenium ç™¼ç”Ÿå•é¡Œï¼š{e}")
            traceback.print_exc()

    driver.quit()
    print("\nâ˜†â˜…â˜† 1111 è·ç¼ºçˆ¬å–å®Œæˆ â˜†â˜…â˜†")

if __name__ == "__main__":
    run_1111_scraping()
